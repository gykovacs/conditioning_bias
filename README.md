# Implementation of the study "The Conditioning Bias in Binary Decision Trees and Random Forests and Its Elimination"

## Introduction

In this repository, we share the all the implementations required for the complete reproduction of the results presented in the study.

Citation:

```bibtex
@article{conditioning-bias,
    author = {G\'abor T\'im\'ar and Gy\"orgy Kov\'acs},
    title = {The Conditioning Bias in Binary Decision Trees and Random Forests and Its Elimination},
    year = {2023}
}
```

## Contents

The contents of the repository:

* `conditioning_bias`: the Python package containing the implementations of the proposed methods;
* `study`: the notebooks that can be used to reproduce the results of the study.

## Reproducing the results

In this section, we provide a detailed description of how to reproduce the study.

### Prerequisites

First, clone the repository:

```bash
> git clone
```

Create a new Python virtual environment (we use the `conda` environment manager):

```bash
> conda create -n conditioning-bias python==3.11 jupyter ipykernel
```

Activate the environment:

```bash
> conda activate conditioning-bias
```

Enter the repository and install the package:

```bash
> cd conditioning-bias
> pip install .
```

### Executing the steps of the analysis

The study consists of multiple steps, each generating .csv files containing the results. These data files are too large for a code repository, therefore, they are not added. There are two options for the reproduction:

* One can download the data files from the link https://drive.google.com/file/d/1HXWi-sMyJaSoExeLb7h1wdoAEQufEY-C/view?usp=sharing and copy them into the `study/data` folder. Then, any step of the study (see below) can be executed.
* One can execute each step of the study, and regenerate these data files. Dut to the fixed random seeds, all results should be identical.

#### Jupyter

The main steps of the study are arranged as individual Jupyter notebooks. In order to execute them, fire up Jupyter or any IDE supporting the notebook, and activate the `conditioning-bias` kernel to be used.

#### Configuration

The entire experiment can be configured by overwriting the variables in the [study/config.py](study/config.py) module.

#### Datasets

The datasets are loaded and filtered through the tools of the `common-datasets` package.  The filtering parameters can be adjusted in the `study/config.py` module. For the ease of use, some preprocessing is applied in the `study/datasets.py` module.

#### Step 0 - The decision tree used for illustration

The decision tree used for illustration in the paper is constructed in the notebook `study/000-illustration-decision-tree.ipynb`

#### Step 1 - Model selection

In this step, the cross-validated model selection is carried out. The notebooks `study/001-model-selection-md.ipynb` and `study/001-model-selection-msl.ipynb` execute the evaluation on a range of `max_depth` and `min_samples_leaf` parameters. Then, the `study/001-optimal-parameters.ipynb` notebook summarizes the results and determines the best hyperparameter for each dataset. The results are saved in the configurable `data_dir` folder as .csv files. The abberviations `dtc`, `dtr`, `rfc` and `rfr` refer to decision tree classification, decision tree regression, random forest classification and random forest regression, respectively.

#### Step 2 - Generating the summary of the datasets

The summary of the datasets (including the selected hyperparameters) is generated by the notebook `study/002-dataset-summary.ipynb`. The results are Latex files in the configurable `tab_dir` folder.

#### Step 3 - Checking the attributes of the toy datasets in `sklearn`

The notebook `study/003-sklearn-datasets.ipynb` checks the features of the toy datasets available in the `sklearn` package.

#### Step 4 - Generating the statistics of nodes with thresholds on domain values

The notebook `study/004-lattice-split-counts.ipynb` determines the proportions of decision tree nodes with thresholds falling on feature domain values. The results are .csv files in the configurable `data_dir` folder.

#### Step 5 - The cross-validated evaluation of all variations of the estimators

The notebook `study/005-evaluation.ipynb` carries out the extensive evaluation of all variations of the estimators. By default, 400 times repeated 5-fold cross-validation is carried out, however, the parameters of the evaluation are configurable. The results are generated as .csv files in the `data_dir` folder. The execution time of the notebook is about 8 hours on ordinary computing equipment.

#### Step 6 - The distributions of AUC and r2 scores

The notebook `study/006-distributions.ipynb` generates the illustration figures for the distributions of the AUC and r2 scores on certain datasets. The figures are saved in the configurable `figure_dir` folder.

#### Step 7 - The analysis of the results regarding the existence of the bias

The analysis of the results regarding the existence of the conditioning bias is carried out in the notebook `study/007-analysis-presence.ipynb`. The result is a Latex table produced in the configurable `tab_dir` folder.

#### Step 8 - The analysis of the results regarding the performance of the proposed method

The analysis of the results of the proposed method is carried out in the notebook `study/008-analysis-proposed-method.ipynb`. The result is a Ltex table produced in the configurable `tab_dir` folder.
